{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "#### Calculate NSE for all train and test periods of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "# Number of validation boreholes (not used in training or testing)\n",
    "num_validation = 4\n",
    "# Sequence length also acts similarly to the warm up period in conventional models\n",
    "seq_length = 730\n",
    "# Portion of data for training, from which test proportion is inferred\n",
    "train_split = 0.8\n",
    "# Batch size should be a exponent of base 2\n",
    "batch_size = 512\n",
    "# Hidden size of LSTM\n",
    "hidden_size = 5\n",
    "# Number of stacked layers of LSTM\n",
    "num_layers = 5\n",
    "# Embedding size\n",
    "embedding_size = 4\n",
    "# Initial learning rate\n",
    "lr = 0.0001\n",
    "# Number of epochs\n",
    "epochs = 100\n",
    "# Learning rate scheduler patience\n",
    "patience = 20\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data\n",
    "def load_data(file_path: str|Path, prefix: str) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and concatenate CSV files from a directory into a DataFrame.\n",
    "\n",
    "    This function reads all CSV files in the specified directory that match the provided prefix, \n",
    "    parses the \"Date\" column into datetime format with day first, and concatenates the data into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (str|Path): The directory path where the CSV files are located.\n",
    "        prefix (str): The prefix of the CSV files to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The concatenated DataFrame of all matched CSV files.\n",
    "\n",
    "    \"\"\"\n",
    "    return pd.concat(\n",
    "        [\n",
    "            pd.read_csv(p, parse_dates=[\"Date\"], dayfirst=True)\n",
    "            for p in Path(file_path).glob(f\"{prefix}*.csv\")\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in observed groundwater and meteorological data\n",
    "df_gwl = load_data(\"../data_processed/\", \"AquiMod_\")\n",
    "df_met = load_data(\"../data_processed/\", \"ukcp18_\")\n",
    "# Generate incremental borehole_id\n",
    "df_gwl['bhid'] = (df_gwl['Borehole'] != df_gwl['Borehole'].shift()).cumsum() - 1\n",
    "num_boreholes = df_gwl[\"bhid\"].max() + 1\n",
    "num_training = num_boreholes - num_validation\n",
    "# Merge data\n",
    "df_data = pd.merge(left=df_gwl, right=df_met, on=[\"Borehole\", \"Model\", \"Date\"], how=\"inner\").dropna().reset_index(drop=True)\n",
    "# Save memory\n",
    "del df_gwl\n",
    "del df_met\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into separate dataframes for training, testing and validation\n",
    "# Fit GWL scalers\n",
    "# Transform GWL data with scalers\n",
    "# Create sequences\n",
    "# I have just realised that the validation data needs to include the final seq_length values from the training data\n",
    "\n",
    "df_train_test = df_data.query(f\"bhid < {num_training}\")\n",
    "# df_validation = df_data.query(f\"bhid >= {num_training}\")\n",
    "train_list = []\n",
    "test_list = []\n",
    "# Loop through training boreholes\n",
    "for i in range(num_training):\n",
    "    # Slice dataframe to borehole\n",
    "    df = df_train_test.query(\"bhid == @i\").copy()\n",
    "    # Split data into training and testing\n",
    "    train_size = int((len(df) - seq_length) * train_split)\n",
    "    train_list.append(df.iloc[:train_size])\n",
    "    test_list.append(df.iloc[train_size:])\n",
    "\n",
    "df_train = pd.concat(train_list)\n",
    "df_test = pd.concat(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise scalers\n",
    "precip_scaler = StandardScaler()\n",
    "pet_scaler = StandardScaler()\n",
    "gwl_scalers = [StandardScaler() for _ in range(num_boreholes)]\n",
    "\n",
    "# Fit and transform borehole-independent scalers\n",
    "precip_train = precip_scaler.fit_transform(df_train[\"precipwsnow\"].values.reshape(-1, 1))\n",
    "precip_test = precip_scaler.transform(df_test[\"precipwsnow\"].values.reshape(-1, 1))\n",
    "pet_train = pet_scaler.fit_transform(df_train[\"PET\"].values.reshape(-1, 1))\n",
    "pet_test = pet_scaler.transform(df_test[\"PET\"].values.reshape(-1, 1))\n",
    "# Extract bhid data\n",
    "bhid_train = df_train[\"bhid\"].values.reshape(-1, 1)\n",
    "bhid_test = df_test[\"bhid\"].values.reshape(-1, 1)\n",
    "\n",
    "# Fit and transform borehole scalers\n",
    "gwl_train = []\n",
    "gwl_test = []\n",
    "\n",
    "for i in range(num_training):\n",
    "    scaler = gwl_scalers[i]\n",
    "    gwl_train.append(scaler.fit_transform(df_train[df_train[\"bhid\"] == i][\"Obs\"].values.reshape(-1, 1)))\n",
    "    gwl_test.append(scaler.transform(df_test[df_test[\"bhid\"] == i][\"Obs\"].values.reshape(-1, 1)))\n",
    "\n",
    "gwl_train = np.vstack(gwl_train)\n",
    "gwl_test = np.vstack(gwl_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the final (seq_length - 1) number of timesteps to the testing data to generate continous sequences\n",
    "# I need to confirm this but I think we need to add (seq_length - 1) instead of (seq_length)\n",
    "# This is because prepending the full seq_length would create an entire timestep within the training data\n",
    "# Ultimately, it is only one day and doesn't actually matter much\n",
    "precip_test = np.concatenate((precip_train[-(seq_length - 1):], precip_test), axis=0)\n",
    "pet_test = np.concatenate((pet_train[-(seq_length - 1):], pet_test), axis=0)\n",
    "bhid_test = np.concatenate((bhid_train[-(seq_length - 1):], bhid_test), axis=0)\n",
    "gwl_test = np.concatenate((gwl_train[-(seq_length - 1):], gwl_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data: np.ndarray, seq_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Transforms 2D time-series data into an array of sequences of a specified length.\n",
    "\n",
    "    Parameters:\n",
    "    data (np.ndarray): A 2D numpy array where each row is a time step and each column is a feature.\n",
    "    seq_length (int): The number of time steps to include in each output sequence.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 3D numpy array of shape (num_samples - seq_length + 1, seq_length, num_features).\n",
    "    \"\"\"\n",
    "\n",
    "    xs = []  # Initialise an empty list to store sequences\n",
    "\n",
    "    # For each possible sequence in the data...\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        # Extract a sequence of length `seq_length`\n",
    "        x = data[i: (i + seq_length)]\n",
    "        # Append the sequence to the list\n",
    "        xs.append(x)\n",
    "\n",
    "    # Convert the list of sequences into a 3D numpy array\n",
    "    return np.array(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_sequences has to be called individually on each timeseries from each borehole\n",
    "# Initialise lists to hold dynamic and static data for each borehole for train and test periods\n",
    "dynamic_train_list = []\n",
    "dynamic_test_list = []\n",
    "static_train_list = []\n",
    "static_test_list = []\n",
    "gwl_train_list = []\n",
    "gwl_test_list = []\n",
    "\n",
    "# Loop through training boreholes in each of the data types and call create_sequences\n",
    "for i in range(num_boreholes):\n",
    "    train_mask = (bhid_train == i)\n",
    "    test_mask = (bhid_test == i)\n",
    "    dynamic_train_list.append(\n",
    "        torch.from_numpy(\n",
    "            create_sequences(\n",
    "                np.column_stack((precip_train[train_mask], pet_train[train_mask])), seq_length\n",
    "            )\n",
    "        ).float()\n",
    "    )\n",
    "    dynamic_test_list.append(\n",
    "        torch.from_numpy(\n",
    "            create_sequences(\n",
    "                np.column_stack((precip_test[test_mask], pet_test[test_mask])), seq_length\n",
    "            )\n",
    "        ).float()\n",
    "    )\n",
    "    static_train_list.append(torch.from_numpy(create_sequences(bhid_train[train_mask], seq_length)))\n",
    "    static_test_list.append(torch.from_numpy(create_sequences(bhid_test[test_mask], seq_length)))\n",
    "    gwl_train_list.append(\n",
    "        torch.from_numpy(gwl_train[train_mask][seq_length - 1:].reshape(-1, 1)).float()\n",
    "    )\n",
    "    gwl_test_list.append(\n",
    "        torch.from_numpy(gwl_test[test_mask][seq_length - 1:].reshape(-1, 1)).float()\n",
    "    )\n",
    "\n",
    "# dynamic_train_arr = np.concatenate(dynamic_train_list)\n",
    "# dynamic_test_arr = np.concatenate(dynamic_test_list)\n",
    "# static_train_arr = np.concatenate(static_train_list)\n",
    "# static_test_arr = np.concatenate(static_test_list)\n",
    "# gwl_train_arr = np.concatenate(gwl_train_list)\n",
    "# gwl_test_arr = np.concatenate(gwl_test_list)\n",
    "\n",
    "# # Save memory\n",
    "# del dynamic_train_list\n",
    "# del dynamic_test_list\n",
    "# del static_train_list\n",
    "# del static_test_list\n",
    "# del gwl_train_list\n",
    "# del gwl_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, dynamic_sequence, static_data):\n",
    "    \"\"\"\n",
    "    Perform inference with the trained LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained LSTM model\n",
    "    - dynamic_sequence: Input dynamic sequence (shape: [sequence_length, dynamic_size])\n",
    "    - static_data: Input static data (shape: [sequence_length])\n",
    "\n",
    "    Returns:\n",
    "    - predictions: Model predictions (shape: [sequence_length, output_size])\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure that dynamic_sequence has the shape [batch_size, sequence_length, dynamic_size]\n",
    "    # Ensure that static_data has the shape [batch_size, sequence_length]\n",
    "\n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        predictions = model(dynamic_sequence, static_data)\n",
    "\n",
    "    return predictions.cpu().numpy()  # Convert predictions to a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
